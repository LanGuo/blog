## BLEU

Bilingual evaluation understudy (or BLEU) is a metric for evaluating the quality of machine translation using human translation as examples of acceptable quality results.  This metric has become a widely used standard in the research literature.  But is it the perfect measure of quality of machine translation?  This episode explores that idea.

### Related Links

* [Chris Callison-Burch](http://www.cis.upenn.edu/~ccb/)
* [Chris on Twitter](https://twitter.com/ccb)
* [Moses](http://www.statmt.org/moses/)
* [BLEU: a Method for Automatic Evaluation of Machine Translation](http://aclweb.org/anthology/P/P02/P02-1040.pdf)
* [Re-evaluating the Role of BLEU in Machine Translation Research](http://www.cs.jhu.edu/~ccb/publications/re-evaluating-the-role-of-bleu-in-mt-research.pdf)
* [ACL 2019, Fourth conference on machine translation (WMT19)](http://www.statmt.org/wmt19/)

