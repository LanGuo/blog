# Journal Club, episode 9

Kyle: How can AI help in a humanitarian crisis? https://www.independent.co.uk/news/science/artifical-intelligence-disaster-response-humanitarian-crisis-ai-help-a8319361.html

Lan talks about [Captum](https://medium.com/pytorch/introduction-to-captum-a-model-interpretability-library-for-pytorch-d236592d8afa), an extensive interpretability library for PyTorch models.

George's paper this week is [Sanity Checks for Saliency Maps](https://arxiv.org/abs/1810.03292). This work takes stock of a group of techniques that generate local interpretabilty - and assesses their trustworthiness through two 'sanity checks'. From this analysis, [Adebayo](https://twitter.com/julius_adebayo) et al demonstrate that a number of these tools are invariant to the model's weights and could lead a human observer into confirmation bias.
