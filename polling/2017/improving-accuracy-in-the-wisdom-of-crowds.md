## Improving accuracy in the wisdom of crowds

In a [letter to Nature](http://www.nature.com/nature/journal/v75/n1949/abs/075450a0.html) published in 1907, Francis Galton described an event that had taken place at a county fair, where he asked roughly 800 people to guess the weight of an ox. The average estimate was 1,197 pounds. The actual weight was 1,198, which meant that the average guess was a near-perfect estimate. Many people who participated from the crowd were considered experts, such as farmers and butchers, but many people were non-experts who were just attending the fair. Also, none of them guessed the correct weight, and only one person guessed 1,197. The next closest guess was 1,199, which was given by two participants.  

The “wisdom of the crowds” is the idea that errors of judgment should cancel each other out in a large group of people. If one were to ask a group of individuals to guess a random stranger’s height, an average of the answers should be close to correct, based on past statistical arguments on crowd wisdom.

This concept was further popularized in James Surowiecki’s book “The Wisdom of Crowds,” where he argues that crowds make good predictions when the makeup of the crowd satisfies four conditions. One, the people in the group represent a diversity of opinion. Second, individuals can think for themselves before exchanging their views. Third, people are able to draw on local knowledge. And finally, there’s some way to aggregate private opinion into a collective decision.

But the wisdom of crowds has its limitations.  In situations where specialized knowledge is needed to identify the correct answer, the presence of experts in the crowd might be drowned out by majority ignorance. How can we extract the best answer from a crowd, especially in the case where both the most popular answer is wrong? 

Alternatives have been proposed, such as obtaining people's confidence ratings. Answers with higher confidence can then be given more weight to improve accuracy. However, confidence ratings fail in some situations, such as when questions are deliberately misleading. A 2004 [study by Drazen Prelec and H. Sebastian Seung](http://www.eecs.harvard.edu/cs286r/courses/fall10/papers/Prelec10.pdf) showed that it is possible to set up surveys where the incorrect answers are also the most popular answers, which are also typically associated with high confidence. Their method relied on asking questions in pairs: the first queried the individual; the second asked for an estimation of how many other respondents would give the same response. They found that people telling the truth tend to overestimate how common their own answer was. 

Confidence ratings don't solve the "wisdom of crowds" problem. Prelec and Seung along with John McCoy revisited the "wisdom of crowds" problem in a recent study published earlier this year in [Nature](http://www.nature.com/nature/journal/v541/n7638/abs/nature21054.html). They propose a method based on a novel definition of the best answer: it is the one offered by those who would be least surprised by the actual answer when disclosed. 

Consider a case from the study, where a group of people were given the following question: “Philadelphia the capital of Pennsylvania: True or False?” Most people gave the incorrect answer (“True”). Why was the majority wrong? Prelec et al. reasoned that someone who only knows that Philadelphia is an important city in Pennsylvania could reasonably conclude that Philadelphia is the capital. Individuals who answer “False” might possess a crucial additional knowledge, that the actual capital is Harrisburg. Moreover, those who know that Philadelphia is not the capital of Pennsylvania can imagine that many others will incorrectly think that way. 

Prelec et al. devised a clever yet simple formula, called the [“surprisingly popular” solution](http://www.nature.com/nature/journal/v541/n7638/fig_tab/nature21054_F2.html) to the single question crowd wisdom problem. In addition to asking respondents, “Philadelphia is the capital of Pennsylvania, Yes or No?” respondents were also requested to assign a confidence rating and answer “What percentage of people in this survey will answer ‘yes’ to this question?” The key here is to compare the actual responses with the predicted responses.

Image: http://www.nature.com/nature/journal/v541/n7638/fig_tab/nature21054_F1.html

In the case where researchers asked, "Is Columbia is the capital of South Carolina, Yes or No?" respondents who answered “Yes” believed that the majority would agree with them. Those who answered “No” also predicted that the majority of people would also reply "No.” In other words, both groups believed that others would give the same answer as theirs.

The researchers got a different result in the example where people were asked, "Is Philadelphia is the capital of Pennsylvania, Yes or No?". Philadelphia is the capital of Pennsylvania there was a difference. While those who replied incorrectly "Yes" also predicted that most people would agree with them, the people who gave the correct response "No" also feel that the most others would reply incorrectly. 

In the Columbia example, there was no surprisingly popular answer in the Columbia example, so what the majority predicted to be the percentage of people replying “Yes” corresponded with the actual number of responses.  However, in the Philadelphia example, the proportion of the group that most people thought would answer incorrectly was significantly less than the actual percentage of individuals that made the incorrect response. Although both the misguided majority and the correct minority predicted that everyone else would give the incorrect response, the team found that the minority (but correct) answer was given much more often than predicted. The answers that are selected surprisingly often, compared to what participants on average expected, are more likely to be accurate.  

The team tested their method in four different settings, where the crowd-based approach ignores specialized knowledge. The first experiment focused on 50 questions about US state capitals. The second included 80 True/False general knowledge questions, in which people were asked to include questions most would answer correctly and those the majority would get wrong. For the third scenario, dermatologists were given 80 images of skin lesions and asked to rate their confidence on what they thought was malign or malignant and predict the distribution of other dermatologists' judgments. In their final experiment, the researchers asked a group of art professionals and a group of MIT students who had never taken art courses to evaluate the market value of 90 reproductions of contemporary artworks. Participants were asked to predict the price of each artwork, and then estimate the percentage of people predicting a price more than $30,000.

In all four scenarios, the “surprisingly popular” method out-performed crowd wisdom or confidence-based methods alone, reducing errors by between 21 and 35 percent. They found that people are often as confident in an incorrect answer as the correct one; hence, confidence ratings are not always useful. Rather than weighing people equally like in crowd wisdom, their solution takes into account the knowledge disparity between respondents who know the right answer and those who do not. 

The surprisingly popular approach seems to be a compelling approach to the nuances in the wisdom of the crowd. It recognizes that in cases where there are large differences in specialized knowledge in a crowd with differing opinions, the more-informed minorities could override the crowd opinion.
